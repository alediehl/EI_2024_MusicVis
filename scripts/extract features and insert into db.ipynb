{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31349309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #271: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "from visual_clutter import Vlc\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c17df",
   "metadata": {},
   "source": [
    "This notebook gets a video and extracts features from it and inserts the results of the analysis into the db_concert_analyses table. \n",
    "\n",
    "Prerequisites:\n",
    "- Having the database up and running\n",
    "- Having related concert and movie entries in db_concerts & db_movies tables. As the metadata regarding the concerts and songs are not structured, these entries must be done by hand so far.\n",
    "- Knowing the id of the inserted \"movie\" in the db_movies table. The movie refers to a song video in our case. The id will be used to connect the analysis to the movie.\n",
    "- Knowing the path of the song video\n",
    "- Knowing the path to VIAN folder to add screenshots (not a must)\n",
    "\n",
    "Library Requirements:\n",
    "- Essentia: I have manually installed it following (https://essentia.upf.edu/installing.html). Tensorflow module of essentia is necessary!\n",
    "- librosa: https://librosa.org/doc/latest/install.html\n",
    "- visual-clutter: https://github.com/kargaranamir/visual-clutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f70a7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"data/Got Me Under Pressure.mp4\"\n",
    "vian_path = \"/Users/uensal/Documents/melike/livemusicvis/VIAN-Web2\"\n",
    "#id of the related movie in the db_movies table\n",
    "id = 560"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480eac0f",
   "metadata": {},
   "source": [
    "Since the structure of the data archive is not certain I only left this example piece of code to iterate over a folder and its subdirectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bdf23d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./classes.txt\n",
      "./.DS_Store\n",
      "./similarity metric calculation.ipynb\n",
      "./extract features and insert into db.ipynb\n",
      "./audioset-yamnet-1.pb\n",
      "./metadata analysis (to be run after features).ipynb\n",
      "./EfficientNetB3-instruments-99.33.h5\n",
      "./instruments.csv\n",
      "./class_dict.csv\n",
      "./.ipynb_checkpoints/extract features and insert into db-checkpoint.ipynb\n"
     ]
    }
   ],
   "source": [
    "rootdir = './'\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        print(os.path.join(subdir, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# from https://github.com/chuckcho/camera-motion-detection/blob/master/cam_detect.py\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "try:\n",
    "    import cv2\n",
    "    # normalize some property names across opencv versions\n",
    "    try:\n",
    "        from cv2 import CAP_PROP_FPS\n",
    "        from cv2 import CAP_PROP_FRAME_COUNT\n",
    "    except ImportError:\n",
    "        from cv2.cv import CV_CAP_PROP_FPS as CAP_PROP_FPS\n",
    "        from cv2.cv import CV_CAP_PROP_FRAME_COUNT as CAP_PROP_FRAME_COUNT\n",
    "except ImportError as import_error:\n",
    "    #LOGGER.info('%s | calculate_motion_and_jitterness: Running on a non-CUDA '\n",
    "    print('[error] calculate_motion_and_jitterness: Running on a non-CUDA '\n",
    "        'server.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a993787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/chuckcho/camera-motion-detection/blob/master/cam_detect.py\n",
    "def find_dominant_mag_ang(flow):\n",
    "    \"\"\"\n",
    "    Find a dominant magnitude and angle given optical flow map\n",
    "    \"\"\"\n",
    "    mag_map, ang_map = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "    # If mean(mag) >= thresh1 and std(mag) <= thresh2, this magnitude is\n",
    "    # considered \"dominant\"\n",
    "    min_mag_mean = 0.05 * mag_map.shape[0]/50\n",
    "    # max mag std deviation relative to mean\n",
    "    max_mag_std = 1.0\n",
    "    max_ang_std = 0.8\n",
    "\n",
    "    #mag_mean = np.mean(mag_map)\n",
    "    mag_mean = np.mean(mag_map)\n",
    "    mag_std = np.std(mag_map)\n",
    "    if mag_mean >= min_mag_mean and mag_std <= max_mag_std * mag_mean:\n",
    "        dom_mag = mag_mean\n",
    "    else:\n",
    "        dom_mag = float('nan')\n",
    "\n",
    "    # If std(ang) <= thresh3, this angle is considered \"dominant\"\n",
    "    # Take cos() to wrap inherently circular angle (0~2*pi, 0=2*pi)\n",
    "    ang_std = np.std(np.cos(ang_map))\n",
    "    if ang_std <= max_ang_std:\n",
    "        #dom_ang = np.mean(ang_map) * 180 / np.pi\n",
    "        dom_ang = np.median(ang_map) * 180 / np.pi\n",
    "    else:\n",
    "        dom_ang = float('nan')\n",
    "\n",
    "    # Only if both dom_mag and dom_ang are good, this frame is good\n",
    "    #if math.isnan(dom_mag):\n",
    "    #    dom_ang = float('nan')\n",
    "    #if math.isnan(dom_ang):\n",
    "    #    dom_mag = float('nan')\n",
    "\n",
    "    return dom_mag, dom_ang\n",
    "\n",
    "def detect_pan_tilt_zoom(videofile, OF_overlay_videofile=None):\n",
    "    \"\"\"\n",
    "    Detect Pan/Tilt/Zoom camera motion separately\n",
    "    \"\"\"\n",
    "\n",
    "    # display images for debugging/troubleshooting\n",
    "    visualize = False\n",
    "    debug = False\n",
    "\n",
    "    # frames per second (skip other frames)\n",
    "    # process only every n-th frame\n",
    "    sampling_rate = 1\n",
    "\n",
    "    # image resize ratio\n",
    "    resize_ratio = 0.5\n",
    "\n",
    "    # will ignore short segments of frames in motion (likely to be noisy)\n",
    "    min_consecutive_frames = 5\n",
    "\n",
    "    # get FPS\n",
    "    cap = cv2.VideoCapture(videofile)\n",
    "    fps = cap.get(CAP_PROP_FPS)\n",
    "\n",
    "    # if unavailable, by default 30.0\n",
    "    if fps <= 0.0 or math.isnan(fps):\n",
    "        fps = 30.0\n",
    "\n",
    "    # read first frame and resize\n",
    "    ret, frame1 = cap.read()\n",
    "    frame1 = cv2.resize(frame1, (0, 0), fx=resize_ratio, fy=resize_ratio)\n",
    "    previous_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if visualize:\n",
    "        hsv = np.zeros_like(frame1)\n",
    "        hsv[..., 1] = 255\n",
    "\n",
    "    frame_nums = []\n",
    "    timestamps = []\n",
    "    cummulative_dom_mag = []\n",
    "    cummulative_dom_ang = []\n",
    "    frame_num = 1\n",
    "    count = 1\n",
    "\n",
    "    if visualize:\n",
    "        plot_window_size = 500\n",
    "        cv2.namedWindow('original', cv2.WINDOW_NORMAL)\n",
    "        cv2.namedWindow('optical flow', cv2.WINDOW_NORMAL)\n",
    "        cv2.namedWindow('dominant mag(OF)', cv2.WINDOW_NORMAL)\n",
    "        cv2.namedWindow('dominant ang(OF)', cv2.WINDOW_NORMAL)\n",
    "\n",
    "\n",
    "\n",
    "    while 1:\n",
    "        # read subsequent frame\n",
    "        ret, frame2 = cap.read()\n",
    "\n",
    "        # check for end of video\n",
    "        if not ret:\n",
    "            if visualize:\n",
    "                k = cv2.waitKey(0)\n",
    "            break\n",
    "\n",
    "        # skip frames\n",
    "        #if frame_num % int(round(fps/sampling_rate)) != 0:\n",
    "        if frame_num % sampling_rate != 0:\n",
    "            frame_num += 1\n",
    "            continue\n",
    "\n",
    "        # resize\n",
    "        frame2 = cv2.resize(frame2, (0, 0), fx=resize_ratio, fy=resize_ratio)\n",
    "        next_frame = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # get optical flow\n",
    "        # refer to http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowfarneback\n",
    "        # for details about each parameter\n",
    "        flow = cv2.calcOpticalFlowFarneback(\n",
    "                prev=previous_frame,\n",
    "                next=next_frame,\n",
    "                flow=None,\n",
    "                pyr_scale=0.5,\n",
    "                levels=3,\n",
    "                winsize=15,\n",
    "                iterations=3,\n",
    "                poly_n=5,\n",
    "                poly_sigma=1.2,\n",
    "                flags=0\n",
    "                )\n",
    "\n",
    "        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "        if visualize:\n",
    "            hsv[..., 0] = ang*180/np.pi/2\n",
    "            hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        # find majority angle and magnitute\n",
    "        dom_mag, dom_ang = find_dominant_mag_ang(flow)\n",
    "\n",
    "        cummulative_dom_mag.append(dom_mag)\n",
    "        cummulative_dom_ang.append(dom_ang)\n",
    "        timestamp = frame_num / fps\n",
    "        frame_nums.append(frame_num)\n",
    "        timestamps.append(timestamp)\n",
    "\n",
    "        previous_frame = next_frame\n",
    "        frame_num += 1\n",
    "        count += 1\n",
    "\n",
    "         # if enabled, will display (1) original image, (2) optical flow image,\n",
    "        # and (3) history of dominant optical flow angles\n",
    "        if visualize:\n",
    "            cummulative_dom_mag_img = np.zeros(\n",
    "                                        (180, plot_window_size, 3),\n",
    "                                        np.uint8\n",
    "                                        )\n",
    "            for i in range(max(0, count - plot_window_size), count):\n",
    "                cv2.circle(\n",
    "                    cummulative_dom_mag_img,\n",
    "                    (\n",
    "                    count-i,\n",
    "                    cummulative_dom_mag_img.shape[0] - max(\n",
    "                            int(cummulative_dom_mag[i])*10, 0)\n",
    "                    ),\n",
    "                    1, (0, 0, 255), 1)\n",
    "            cummulative_dom_ang_img = np.zeros(\n",
    "                                        (180, plot_window_size, 3),\n",
    "                                        np.uint8\n",
    "                                        )\n",
    "            for i in range(max(0, count - plot_window_size), count):\n",
    "                cv2.circle(\n",
    "                    cummulative_dom_ang_img,\n",
    "                    (\n",
    "                    count-i,\n",
    "                    cummulative_dom_ang_img.shape[0] - max(\n",
    "                            int(cummulative_dom_ang[i]), 0)\n",
    "                    ),\n",
    "                    1, (0, 0, 255), 1)\n",
    "            cv2.imshow('original', frame2)\n",
    "            cv2.imshow('optical flow', bgr)\n",
    "            cv2.imshow('dominant mag(OF)', cummulative_dom_mag_img)\n",
    "            cv2.imshow('dominant ang(OF)', cummulative_dom_ang_img)\n",
    "\n",
    "            k = cv2.waitKey(30) & 0xff\n",
    "            if k == 27:\n",
    "                break\n",
    "\n",
    "        previous_frame = next_frame\n",
    "        frame_num += 1\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # detect pan/tilt/zoom for each frame from dom_mag and dom_ang's (only\n",
    "    # if they persists in some consecutive frames)\n",
    "\n",
    "    # dealing with numpy array is easier than python list\n",
    "    cummulative_dom_mag = np.array(cummulative_dom_mag)\n",
    "    cummulative_dom_ang = np.array(cummulative_dom_ang)\n",
    "    pan = np.array([False] * len(frame_nums))\n",
    "    tilt = np.array([False] * len(frame_nums))\n",
    "    zoom = np.array([False] * len(frame_nums))\n",
    "\n",
    "    for count, frame in enumerate(frame_nums[:-(min_consecutive_frames-1)]):\n",
    "        if all(np.isfinite(\n",
    "                cummulative_dom_ang[count:count+min_consecutive_frames]\n",
    "                )) and all(np.isfinite(\n",
    "                cummulative_dom_mag[count:count+min_consecutive_frames]\n",
    "                )):\n",
    "            this_clip_pan_or_tilt = True\n",
    "        else:\n",
    "            this_clip_pan_or_tilt = False\n",
    "        if this_clip_pan_or_tilt:\n",
    "            # check if dominant angle was vertical (tilt) or horizontal (pan)\n",
    "\n",
    "            #mean_dom_ang = np.mean(cummulative_dom_ang[count:count+min_consecutive_frames])\n",
    "            mean_dom_ang = np.median(cummulative_dom_ang[count:count+min_consecutive_frames])\n",
    "            std_dom_ang = np.std(cummulative_dom_ang[count:count+min_consecutive_frames])\n",
    "            if std_dom_ang > 35:\n",
    "                # don't tag this frame with either tilt nor pan\n",
    "                pass\n",
    "            if (45 + 20 <= mean_dom_ang <= 135 - 20) or (225 + 20 <= mean_dom_ang <= 315 - 20):\n",
    "                tilt[count:count+min_consecutive_frames] = True\n",
    "                pan[count:count+min_consecutive_frames] = False\n",
    "            else:\n",
    "                tilt[count:count+min_consecutive_frames] = False\n",
    "                pan[count:count+min_consecutive_frames] = True\n",
    "\n",
    "        else:\n",
    "            mean_dom_ang = np.nan\n",
    "\n",
    "        if debug:\n",
    "            print(\"[debug] f={}, t={}, dom_mag={}, dom_ang={}, mean_dom_ang={}, pan={}, tilt={}\".format(\n",
    "                    frame,\n",
    "                    timestamps[count],\n",
    "                    cummulative_dom_mag[count],\n",
    "                    cummulative_dom_ang[count],\n",
    "                    mean_dom_ang,\n",
    "                    pan[count],\n",
    "                    tilt[count]))\n",
    "\n",
    "    if OF_overlay_videofile:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter(OF_overlay_videofile, fourcc, fps/sampling_rate*2, frame1.shape[1::-1])\n",
    "\n",
    "        # get FPS\n",
    "        cap = cv2.VideoCapture(videofile)\n",
    "\n",
    "        # read first frame and resize\n",
    "        ret, frame1 = cap.read()\n",
    "        frame1 = cv2.resize(frame1, (0, 0), fx=resize_ratio, fy=resize_ratio)\n",
    "        previous_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        frame_num = 1\n",
    "        count = 0\n",
    "\n",
    "        while 1:\n",
    "            # read subsequent frame\n",
    "            ret, frame2 = cap.read()\n",
    "\n",
    "            # check for end of video\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # skip frames\n",
    "            #if frame_num % int(round(fps/sampling_rate)) != 0:\n",
    "            if frame_num % sampling_rate != 0:\n",
    "                frame_num += 1\n",
    "                continue\n",
    "\n",
    "            # resize\n",
    "            frame2 = cv2.resize(frame2, (0, 0), fx=resize_ratio, fy=resize_ratio)\n",
    "            if OF_overlay_videofile:\n",
    "                tmp_frame = frame2\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                (width, height) = frame2.shape[1::-1]\n",
    "                if tilt[count]:\n",
    "                    cv2.putText(tmp_frame,'Tilt',(10,100), font, 1,(0,0,255),2,cv2.LINE_AA)\n",
    "                elif pan[count]:\n",
    "                    cv2.putText(tmp_frame,'Pan',(10,100), font, 1,(0,255,255),2,cv2.LINE_AA)\n",
    "                out.write(tmp_frame)\n",
    "\n",
    "            next_frame = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # get optical flow\n",
    "            # refer to http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowfarneback\n",
    "            # for details about each parameter\n",
    "            flow = cv2.calcOpticalFlowFarneback(\n",
    "                    prev=previous_frame,\n",
    "                    next=next_frame,\n",
    "                    flow=None,\n",
    "                    pyr_scale=0.5,\n",
    "                    levels=3,\n",
    "                    winsize=15,\n",
    "                    iterations=3,\n",
    "                    poly_n=5,\n",
    "                    poly_sigma=1.2,\n",
    "                    flags=0\n",
    "                    )\n",
    "\n",
    "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "            # overlay OF fields\n",
    "            if OF_overlay_videofile:\n",
    "                pass\n",
    "\n",
    "            previous_frame = next_frame\n",
    "            frame_num += 1\n",
    "            count += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "    return (pan,\n",
    "            tilt,\n",
    "            zoom,\n",
    "            frame_nums,\n",
    "            timestamps,\n",
    "            cummulative_dom_mag,\n",
    "            cummulative_dom_ang, fps)\n",
    "\n",
    "def detection(video_path):\n",
    "    (pan, tilt, zoom, frame_nums, timestamps, dom_mag, dom_ang, fps) = detect_pan_tilt_zoom(video_path, OF_overlay_videofile=None)\n",
    "\n",
    "    # human-friendly print out: video, pan, tilt, zoom\n",
    "    camera_motion_perframe = dict()\n",
    "\n",
    "    'frame_num, time in sec, dominant OF mag, dominant OF ang, pan, tilt, zoom\\n'\n",
    "    for count, frame in enumerate(frame_nums[:-2]):\n",
    "        motions = list()\n",
    "        if pan[count]:\n",
    "            motions.append(\"pan\")\n",
    "        if tilt[count]:\n",
    "            motions.append(\"tilt\")\n",
    "        if zoom[count]:\n",
    "            motions.append(\"zoom\")\n",
    "        # frame_nums[count] = frame number\n",
    "        camera_motion_perframe[frame_nums[count]] = motions\n",
    "        \n",
    "    return camera_motion_perframe, fps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86208da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_motion_perframe, fps = detection(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Classes for instrument detection -> see next lines of code\n",
    "\n",
    "class_index,class,height,width,scale by\n",
    "0,Didgeridoo,224,224,1\n",
    "1,Tambourine,224,224,1\n",
    "2,Xylophone,224,224,1\n",
    "3,acordian,224,224,1\n",
    "4,alphorn,224,224,1\n",
    "5,bagpipes,224,224,1\n",
    "6,banjo,224,224,1\n",
    "7,bongo drum,224,224,1\n",
    "8,casaba,224,224,1\n",
    "9,castanets,224,224,1\n",
    "10,clarinet,224,224,1\n",
    "11,clavichord,224,224,1\n",
    "12,concertina,224,224,1\n",
    "13,drums,224,224,1\n",
    "14,dulcimer,224,224,1\n",
    "15,flute,224,224,1\n",
    "16,guiro,224,224,1\n",
    "17,guitar,224,224,1\n",
    "18,harmonica,224,224,1\n",
    "19,harp,224,224,1\n",
    "20,marakas,224,224,1\n",
    "21,ocarina,224,224,1\n",
    "22,piano,224,224,1\n",
    "23,saxaphone,224,224,1\n",
    "24,sitar,224,224,1\n",
    "25,steel drum,224,224,1\n",
    "26,trombone,224,224,1\n",
    "27,trumpet,224,224,1\n",
    "28,tuba,224,224,1\n",
    "29,violin,224,224,1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60d12e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model from https://www.kaggle.com/code/gpiosenka/explore-instruments-data-set/data\n",
    "model=keras.models.load_model(\"EfficientNetB3-instruments-99.33.h5\") # # https://www.kaggle.com/code/gpiosenka/explore-instruments-data-set\n",
    "class_df=pd.read_csv(\"class_dict.csv\") \n",
    "class_count=len(class_df['class'].unique())\n",
    "img_height=int(class_df['height'].iloc[0])\n",
    "img_width =int(class_df['width'].iloc[0])\n",
    "img_size=(img_width, img_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Classes of interest:\n",
    "class_index,class,height,width,scale by\n",
    "0,Didgeridoo,224,224,1\n",
    "4,alphorn,224,224,1\n",
    "5,bagpipes,224,224,1\n",
    "6,banjo,224,224,1\n",
    "7,bongo drum,224,224,1\n",
    "9,castanets,224,224,1\n",
    "11,clavichord,224,224,1\n",
    "14,dulcimer,224,224,1\n",
    "16,guiro,224,224,1\n",
    "21,ocarina,224,224,1\n",
    "24,sitar,224,224,1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08bfc68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes =[0,4,5,6,7,9,11,14,16,21,24]\n",
    "filtered_classes = [i for j, i in enumerate(class_df[\"class\"]) if j not in indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10c02c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(video_path)\n",
    "fps = cam.get(CAP_PROP_FPS)\n",
    "\n",
    "currentframe = 0\n",
    "clutter_scalars = list()\n",
    "frames = list()\n",
    "\n",
    "musical_instruments = list()\n",
    "inst_frames = list()\n",
    "\n",
    "while True:\n",
    "    ret,frame = cam.read()\n",
    "    if ret: \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, img_size)\n",
    "        pred = model.predict(np.array(list([image])), verbose=0)\n",
    "        pred = pred.flatten()\n",
    "            \n",
    "        filtered_predictions = [i for j, i in enumerate(pred) if j not in indexes]\n",
    "            \n",
    "        test_index=np.argmax(filtered_predictions)\n",
    "        if filtered_predictions[test_index]>=0.5:\n",
    "            musical_instruments.append(test_index)\n",
    "            inst_frames.append(currentframe)\n",
    "            \n",
    "        if currentframe%10==1:\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, (0, 0), fx=0.5, fy=0.5)\n",
    "            clt = Vlc(image, numlevels=3, contrast_filt_sigma=1, contrast_pool_sigma=3, color_pool_sigma=3)\n",
    "            clutter_scalar_fc, clutter_map_fc= clt.getClutter_FC()\n",
    "            clutter_scalars.append(clutter_scalar_fc)\n",
    "            frames.append(currentframe)\n",
    "            \n",
    "        currentframe += 1\n",
    "    else:\n",
    "        break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bf4dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_instruments = list(set(musical_instruments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8769eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "musical_instruments = np.array(musical_instruments)\n",
    "instrument_names = list()\n",
    "for instrument in detected_instruments:\n",
    "    np.place(musical_instruments, musical_instruments==instrument, index)\n",
    "    instrument_names.append(filtered_classes[instrument])\n",
    "    index +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01508b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "camx = []\n",
    "camy = []\n",
    "for (key, value) in camera_motion_perframe.items():\n",
    "    for motion in value:\n",
    "        camx.append(key)\n",
    "        camy.append(motion.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28096d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_arr = []\n",
    "\n",
    "for instrument in musical_instruments:\n",
    "    new_arr.append(instrument_names[instrument])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f620f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "instx = []\n",
    "insty = []\n",
    "\n",
    "for (n, instrument) in enumerate(new_arr):\n",
    "    if new_arr.count(instrument) > len(new_arr)*0.01:\n",
    "        instx.append(inst_frames[n])\n",
    "        if instrument== \"saxaphone\":\n",
    "            instrument = \"Saxophone\"\n",
    "        elif instrument == 'steel drum':\n",
    "            instrument = \"Drums\"\n",
    "        elif instrument == 'acordian':\n",
    "            instrument = \"Accordion\"\n",
    "        elif instrument == 'casaba':\n",
    "            instrument = \"Cabasa\"\n",
    "        insty.append(instrument.title())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ed4d5e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/melikeciloglu/opt/miniconda3/envs/msc/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "y, sr = librosa.load(video_path)\n",
    "sr_resampled = 100\n",
    "y_resampled = librosa.resample(y, orig_sr=sr, target_sr=sr_resampled)\n",
    "y_harm, y_perc = librosa.effects.hpss(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3e019c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "times = librosa.times_like(o_env, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ff009ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_resampled = 2000\n",
    "y_resampled = librosa.resample(y, orig_sr=sr, target_sr=sr_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "940e4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"clutter_scalars\":clutter_scalars,  \"insty\":insty, \"instx\":instx, \n",
    "       \"camx\": camx, \"camy\":camy, \"clutter_frames\": list(np.linspace(0, currentframe, len(clutter_scalars)).astype(float)),\n",
    "       \"y_harm\":list(y_harm.astype(float)), \"y_perc\": list(y_perc.astype(float)), \"onset\":list(o_env.astype(float)), \"onset_times\":list(times.astype(float)),\n",
    "       \"fps\":fps, \"y\":list(y_resampled.astype(float)) , \"sr\":sr_resampled}\n",
    "\n",
    "dumped_json_string = json.dumps(data)\n",
    "binary_data = ' '.join(format(ord(letter), 'b') for letter in dumped_json_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dcdb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "cs = \"dbname=%s user=%s password=%s host=%s port=%s\" % (\"FilmColors_v2_Production\",\"ERCAdmin\",\"admin\",\"localhost\",\"5433\")\n",
    "conn = psycopg2.connect(cs)\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"INSERT INTO public.db_concert_analyses(video_id, classification_object, analysis_class_name, uuid, dtype, shape, data) VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "            ,(id, 'Global', \"MusicAnalysis\",str(uuid.uuid1()), \"dict\", \"\",binary_data))\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccee23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.9/site-packages/') # not permanent\n",
    "import essentia"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec822ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = MonoLoader(filename=video_path, sampleRate=16000)()\n",
    "model = TensorflowPredictVGGish(graphFilename=\"audioset-yamnet-1.pb\", input=\"melspectrogram\", output=\"activations\")\n",
    "activations = model(audio)\n",
    "averaged_predictions = np.mean(activations, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ecf45fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = averaged_predictions.tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9d7bc33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "cs = \"dbname=%s user=%s password=%s host=%s port=%s\" % (\"FilmColors_v2_Production\",\"ERCAdmin\",\"admin\",\"localhost\",\"5432\")\n",
    "conn = psycopg2.connect(cs)\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"INSERT INTO public.db_concert_analyses(video_id, classification_object, analysis_class_name, uuid, dtype, shape, data) VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "            ,(id, 'Global', \"ClassificationAnalysis\",str(uuid.uuid1()), \"np.float64\", \"\",a))\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0971df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92777b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#screenshots\n",
    "cam = cv2.VideoCapture(video_path)\n",
    "fps = cam.get(CAP_PROP_FPS)\n",
    "\n",
    "currentframe = 0\n",
    "\n",
    "while(True):\n",
    "    ret,frame = cam.read()\n",
    "    if ret: \n",
    "        if currentframe%25==1:\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            cv2.imwrite('{}/VIAN-Web2/backend/Base/frames/{}/{}.jpg'.format(vian_path, id, currentframe), image) \n",
    "            \n",
    "        currentframe += 1\n",
    "    else:\n",
    "        break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fce70b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
